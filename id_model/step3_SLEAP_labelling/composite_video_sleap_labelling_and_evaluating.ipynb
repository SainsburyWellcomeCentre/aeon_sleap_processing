{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEON\n",
    "\n",
    "#### create training dataset from aeon raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "import aeon\n",
    "import aeon.io.api as aeon_api\n",
    "\n",
    "from aeon.schema.schemas import exp02, social02\n",
    "from aeon.analysis.utils import *\n",
    "from dotmap import DotMap\n",
    "from scipy import stats\n",
    "\n",
    "import datajoint as dj\n",
    "from aeon.dj_pipeline.analysis.block_analysis import *\n",
    "\n",
    "\n",
    "def get_experiment_times(\n",
    "    root: str | os.PathLike, start_time: pd.Timestamp, end_time: pd.Timestamp\n",
    ") -> DotMap:\n",
    "    \"\"\"\n",
    "    Retrieve experiment start and stop times from environment states\n",
    "    (i.e. times outside of maintenance mode) occurring within the\n",
    "    given start and end times.\n",
    "\n",
    "    Args:\n",
    "        root (str or os.PathLike): The root path where epoch data is stored.\n",
    "        start_time (pandas.Timestamp): Start time.\n",
    "        end_time (pandas.Timestamp): End time.\n",
    "\n",
    "    Returns:\n",
    "        DotMap: A DotMap object containing two keys: 'start' and 'stop',\n",
    "        corresponding to pairs of experiment start and stop times.\n",
    "\n",
    "    Notes:\n",
    "    This function uses the last 'Maintenance' event (if available, otherwise\n",
    "    `end_time`) as the last 'Experiment' stop time. If the first retrieved state\n",
    "    is 'Maintenance' (e.g. 'Experiment' mode entered before `start_time`),\n",
    "    `start_time` is used as the first 'Experiment' start time.\n",
    "    \"\"\"\n",
    "\n",
    "    experiment_times = DotMap()\n",
    "    env_states = aeon.load(\n",
    "        root,\n",
    "        social02.Environment.EnvironmentState,\n",
    "        # aeon.io.reader.Csv(\"Environment_EnvironmentState_*\", [\"state\"]),\n",
    "        start_time,\n",
    "        end_time,\n",
    "    )\n",
    "    if env_states.empty:\n",
    "        warnings.warn(\n",
    "            \"The environment state df is empty. \"\n",
    "            \"Using input `start_time` and `end_time` as experiment times.\"\n",
    "        )\n",
    "        experiment_times.start = [start_time]\n",
    "        experiment_times.stop = [end_time]\n",
    "        return experiment_times\n",
    "    if env_states[\"state\"].iloc[-1] != \"Maintenance\":\n",
    "        warnings.warn(\n",
    "            \"No 'Maintenance' event at the end of the search range. \"\n",
    "            \"Using input `end_time` as last experiment stop time.\"\n",
    "        )\n",
    "        # Pad with a \"Maintenance\" event at the end\n",
    "        env_states = pd.concat(\n",
    "            [\n",
    "                env_states,\n",
    "                pd.DataFrame(\n",
    "                    \"Maintenance\",\n",
    "                    index=[end_time],\n",
    "                    columns=env_states.columns,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    # Use the last \"Maintenance\" event as end time\n",
    "    end_time = (env_states[env_states.state == \"Maintenance\"]).index[-1]\n",
    "    env_states = env_states[~env_states.index.duplicated(keep=\"first\")]\n",
    "    # Retain only events between visit start and stop times\n",
    "    env_states = env_states.iloc[\n",
    "        env_states.index.get_indexer([start_time], method=\"bfill\")[\n",
    "            0\n",
    "        ] : env_states.index.get_indexer([end_time], method=\"ffill\")[0] + 1\n",
    "    ]\n",
    "    # Retain only events where state changes (experiment-maintenance pairs)\n",
    "    env_states = env_states[env_states[\"state\"].ne(env_states[\"state\"].shift())]\n",
    "    if env_states[\"state\"].iloc[0] == \"Maintenance\":\n",
    "        warnings.warn(\n",
    "            \"No 'Experiment' event at the start of the search range. \"\n",
    "            \"Using input `end_time` as last experiment stop time.\"\n",
    "        )\n",
    "        # Pad with an \"Experiment\" event at the start\n",
    "        env_states = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame(\n",
    "                    \"Experiment\",\n",
    "                    index=[start_time],\n",
    "                    columns=env_states.columns,\n",
    "                ),\n",
    "                env_states,\n",
    "            ]\n",
    "        )\n",
    "    experiment_times.start = env_states[\n",
    "        env_states[\"state\"] == \"Experiment\"\n",
    "    ].index.values\n",
    "    experiment_times.stop = env_states[\n",
    "        env_states[\"state\"] == \"Maintenance\"\n",
    "    ].index.values\n",
    "\n",
    "    return experiment_times\n",
    "\n",
    "\n",
    "def exclude_maintenance_data(\n",
    "    data: pd.DataFrame, experiment_times: DotMap\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Excludes rows not in experiment times (i.e., corresponding to maintenance times)\n",
    "    from the given dataframe.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The data to filter. Expected to have a pandas.DateTimeIndex.\n",
    "        experiment_times (DotMap): A DotMap object containing experiment start and stop times.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A pandas DataFrame containing filtered data.\n",
    "    \"\"\"\n",
    "    filtered_data = pd.concat(\n",
    "        [\n",
    "            data.loc[start:stop]\n",
    "            for start, stop in zip(experiment_times.start, experiment_times.stop)\n",
    "        ]\n",
    "    )\n",
    "    return filtered_data\n",
    "\n",
    "def get_raw_tracking_data(\n",
    "    root: str | os.PathLike,\n",
    "    subj_id: str,\n",
    "    start: pd.Timestamp,\n",
    "    end: pd.Timestamp,\n",
    "    source_reader: aeon.io.reader.Video = exp02.CameraTop.Video,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve pos tracking and video data and assigns subject ID.\n",
    "\n",
    "    Args:\n",
    "        root (str or os.PathLike): The root path, or prioritised sequence of paths, where epoch data is stored.\n",
    "        subj_id (str): The subject ID string to be assigned.\n",
    "        start (pandas.Timestamp): The left bound of the time range to extract.\n",
    "        end (pandas.Timestamp): The right bound of the time range to extract.\n",
    "        source_reader (aeon.io.reader.Video, optional): The frame source reader. Default is exp02.CameraTop.Video.\n",
    "    Returns:\n",
    "        pandas.DataFrame: A pandas DataFrame containing pos tracking and video data, and subject ID.\n",
    "    \"\"\"\n",
    "\n",
    "    subj_video = aeon_api.load(root, source_reader, start=start, end=end)\n",
    "    path = Path(root)\n",
    "    acquisition_computer = path.parts[-2].lower()\n",
    "    experiment = path.parts[-1].rstrip('/')\n",
    "    experiment = re.sub(r\"(social0)(\\d+)\", r\"\\1.\\2\", experiment)\n",
    "    experiment_name = f\"{experiment}-{acquisition_computer}\"\n",
    "    key = {\"experiment_name\": experiment_name}\n",
    "    chunk_restriction = {\"chunk_start\": start}\n",
    "    pose_query = (\n",
    "        streams.SpinnakerVideoSource\n",
    "        * tracking.SLEAPTracking.PoseIdentity.proj(\"identity_name\", \"identity_likelihood\", anchor_part=\"part_name\")\n",
    "        * tracking.SLEAPTracking.Part\n",
    "        & {\"spinnaker_video_source_name\": \"CameraTop\"}\n",
    "        & key\n",
    "        & chunk_restriction\n",
    "        & {\"part_name\": \"spine2\"}\n",
    "    )\n",
    "    subj_pos = fetch_stream(pose_query)\n",
    "    # replace \"raw\" in root with \"processed\"\n",
    "    processed_root = root.replace(\"raw\", \"processed\")\n",
    "    if subj_video.empty:\n",
    "        subj_video = aeon_api.load(processed_root, source_reader, start=start, end=end)\n",
    "        warnings.warn(\"subj_video data from raw is empty, retrieving data from processed\")\n",
    "    if subj_pos.empty:\n",
    "        warnings.warn(\"subj_pos data from DJ is empty, retrieving data from processed\")\n",
    "        subj_pos = aeon_api.load(processed_root, aeon.io.reader.Pose(pattern=\"CameraTop_202_gpu-partition*\"), start=start, end=end)\n",
    "        if subj_pos.empty:\n",
    "            warnings.warn(\"subj_pos data from processed is empty, retrieving data from raw\")\n",
    "            subj_pos = aeon_api.load(root, aeon.io.reader.Pose(pattern=\"CameraTop_202*\"), start=start, end=end)\n",
    "            display(subj_pos.sort_index())\n",
    "            if subj_pos.empty:\n",
    "                raise ValueError(\"No tracking data found.\")\n",
    "        subj_pos = subj_pos[subj_pos[\"part\"] == (\"spine2\" if \"spine2\" in subj_pos[\"part\"].values else \"centroid\")]\n",
    "        subj_pos.drop(columns=[\"part\"], inplace=True)\n",
    "        subj_pos.rename(columns={\"identity\": \"identity_name\", \"part_likelihood\": \"likelihood\"}, inplace=True)\n",
    "    subj_data = pd.DataFrame()\n",
    "    for subj in subj_pos[\"identity_name\"].unique():\n",
    "        sample = subj_pos[subj_pos[\"identity_name\"] == subj]\n",
    "        subj_data_group = pd.merge_asof(\n",
    "            subj_video,\n",
    "            sample,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            direction=\"nearest\",\n",
    "            tolerance=pd.Timedelta(\"1ms\"),\n",
    "        )[[\"x\", \"y\", \"identity_name\", \"_frame\", \"_path\"]]\n",
    "        subj_data = pd.concat([subj_data, subj_data_group])\n",
    "    subj_data.dropna(inplace=True)\n",
    "    subj_data[\"distance\"] = np.nan\n",
    "    if \"multi_\" in subj_id:\n",
    "        group_sizes = subj_data.groupby(level=0).size()\n",
    "        if (group_sizes > 2).any():\n",
    "            times_with_more_than_two = group_sizes[group_sizes > 2].index.tolist()\n",
    "            raise ValueError(f\"More than two identities found for time(s): {times_with_more_than_two}\")\n",
    "        # Add a row number within each group to pivot the data\n",
    "        subj_data['row_num'] = subj_data.groupby(level=0).cumcount()\n",
    "        # Pivot the DataFrame to get 'x' and 'y' coordinates for each identity as separate columns\n",
    "        subj_data_pivot = subj_data.pivot_table(\n",
    "            index=subj_data.index.get_level_values(0),\n",
    "            columns='row_num',\n",
    "            values=['x', 'y']\n",
    "        )\n",
    "        # Extract coordinates\n",
    "        x0 = subj_data_pivot['x'][0]\n",
    "        y0 = subj_data_pivot['y'][0]\n",
    "        x1 = subj_data_pivot['x'][1]\n",
    "        y1 = subj_data_pivot['y'][1]\n",
    "        # Calculate distances using vectorized operations\n",
    "        distance = np.sqrt((x0 - x1)**2 + (y0 - y1)**2)\n",
    "        # Map the calculated distances back to the original DataFrame\n",
    "        subj_data['distance'] = subj_data.index.get_level_values(0).map(distance)\n",
    "        subj_data.drop(columns='row_num', inplace=True)\n",
    "    subj_data[\"id\"] = subj_id\n",
    "    # Make x and y columns numeric\n",
    "    subj_data[\"x\"] = pd.to_numeric(subj_data[\"x\"])\n",
    "    subj_data[\"y\"] = pd.to_numeric(subj_data[\"y\"])\n",
    "\n",
    "    return subj_data\n",
    "\n",
    "\n",
    "def sample_n_from_bins(\n",
    "    subj_data: pd.DataFrame,\n",
    "    n_samples: int = 1,\n",
    "    n_bins: int = 50,\n",
    "    range: npt.ArrayLike = [[0, 1440], [0, 1080]],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Uniformly samples n number of data from x number of bins.\n",
    "\n",
    "    Args:\n",
    "        subj_data (pandas.DataFrame): A pandas DataFrame containing pos tracking and video data, and subject ID.\n",
    "        n_samples (int, optional): The number of samples to take from each bin. Default is 1.\n",
    "        n_bins (int, optional): The number of bins to use for sampling. Default is 50.\n",
    "        range (list of lists, optional): The leftmost and rightmost edges of the bins along each dimension\n",
    "            (if not specified explicitly in the bins parameters): [[xmin, xmax], [ymin, ymax]]. All values\n",
    "            outside of this range will be considered outliers and not tallied in the histogram. Default is\n",
    "            [[0, 1440], [0, 1080]].\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A pandas DataFrame containing uniformly-sampled pos tracking and video data, and subject ID.\n",
    "    \"\"\"\n",
    "\n",
    "    hist_data = stats.binned_statistic_2d(\n",
    "        subj_data.x,\n",
    "        subj_data.y,\n",
    "        values=subj_data,\n",
    "        statistic=\"count\",\n",
    "        bins=n_bins,\n",
    "        range=range,\n",
    "    )\n",
    "    subj_data = subj_data.copy()\n",
    "    subj_data[\"bin\"] = hist_data.binnumber\n",
    "    sampled_data = (\n",
    "        subj_data.groupby([\"bin\"]).sample(n=n_samples, replace=True).drop_duplicates()\n",
    "    )\n",
    "    return sampled_data\n",
    "\n",
    "\n",
    "def create_session_dataset(\n",
    "    session: dict,\n",
    "    subj_ids: list = None,\n",
    "    plot_dist: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a dataset for a given session dict.\n",
    "    Args:\n",
    "        session (dict): A dictionary containing the root path, subject IDs, and their start and end times.\n",
    "        subj_ids (list, optional): A list of subject ids. If None, all subjects are selected.\n",
    "        plot_dist (bool, optional): Whether to plot the 1d and 2d histograms of x, y pos tracking for each subject.\n",
    "    Returns:\n",
    "        pandas.DataFrame: A pandas dataframe containing uniformly-sampled pos tracking and video data, and subject ID.\n",
    "    \"\"\"\n",
    "    all_subj_data = pd.DataFrame()\n",
    "    if not subj_ids:\n",
    "        subj_ids = session[\"subjects\"].keys()\n",
    "    for subj in subj_ids:\n",
    "        subj_dict = {\n",
    "            \"id\": subj,\n",
    "            \"root\": session.get(\"root\", session[\"subjects\"][subj].get(\"root\")),\n",
    "            \"start\": session[\"subjects\"][subj][\"start\"],\n",
    "            \"end\": session[\"subjects\"][subj][\"end\"],\n",
    "        }\n",
    "        subj_data = (\n",
    "            create_subject_dataset(\n",
    "                subj_dict,\n",
    "                max_dist = 300,\n",
    "                n_samples=2,\n",
    "                n_bins=50,\n",
    "            )  # sample fewer points for manual annotation\n",
    "            if \"multi_\" in subj\n",
    "            else create_subject_dataset(\n",
    "                subj_dict,\n",
    "            )\n",
    "        )\n",
    "        all_subj_data = pd.concat([all_subj_data, subj_data])\n",
    "    if plot_dist:\n",
    "        fig = plot_position_histograms(all_subj_data)\n",
    "        fig.show()\n",
    "    return all_subj_data\n",
    "\n",
    "\n",
    "def plot_position_histograms(data: pd.DataFrame, n_bins: int = 50) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plots the 1d and 2d histograms of x, y pos tracking for each subject in a given DataFrame.\n",
    "    Args:\n",
    "        data (pandas.DataFrame): A pandas DataFrame containing x, y pos tracking and subject ID(s).\n",
    "        n_bins (int, optional): The number of bins to use for plotting histograms. Default is 50.\n",
    "    Returns:\n",
    "        matplotlib.pyplot: A plot containing 1d and 2d histograms of x, y pos tracking for each subject.\n",
    "    \"\"\"\n",
    "    subj_ids = data[\"id\"].unique()\n",
    "    fig, ax = plt.subplots(2, len(subj_ids))\n",
    "    n_bins = 50\n",
    "    if len(subj_ids) == 1:\n",
    "        data[[\"x\", \"y\"]].plot.hist(bins=n_bins, alpha=0.5, ax=ax[0], title=subj_ids[0])\n",
    "        ax[1].hist2d(\n",
    "            data.x,\n",
    "            data.y,\n",
    "            bins=(n_bins, n_bins),\n",
    "            cmap=plt.cm.jet,\n",
    "        )\n",
    "    else:\n",
    "        for i, subj_id in enumerate(subj_ids):\n",
    "            subj_data = data[data[\"id\"] == subj_id]\n",
    "            subj_data[[\"x\", \"y\"]].plot.hist(\n",
    "                bins=n_bins, alpha=0.5, ax=ax[0, i], title=subj_id\n",
    "            )\n",
    "            ax[1, i].hist2d(\n",
    "                subj_data.x, subj_data.y, bins=(n_bins, n_bins), cmap=plt.cm.jet\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_subject_dataset(\n",
    "    subject: dict,\n",
    "    max_dist: float = None,\n",
    "    n_samples: int = 1,\n",
    "    n_bins: int = 50,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a dataset for a given subject dict.\n",
    "\n",
    "    Args:\n",
    "        subject (dict): A dictionary containing the root path, subject ID, and their start and end times.\n",
    "        min_area (float, optional): The minimum area of the subject to be included in the dataset. Default is None.\n",
    "        n_samples (int, optional): The number of samples to take from each bin. Default is 1.\n",
    "        n_bins (int, optional): The number of bins to use for sampling. Default is 50.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A pandas DataFrame containing uniformly-sampled pos tracking and video data, and subject ID.\n",
    "    \"\"\"\n",
    "    subj_data = get_raw_tracking_data(\n",
    "        subject[\"root\"],\n",
    "        subject[\"id\"],\n",
    "        subject[\"start\"],\n",
    "        subject[\"end\"],\n",
    "    )\n",
    "    if max_dist:\n",
    "        subj_data = subj_data.loc[subj_data.distance <= max_dist]\n",
    "    subj_data_sampled = sample_n_from_bins(subj_data, n_samples=n_samples, n_bins=n_bins)\n",
    "    subj_data = subj_data.loc[subj_data.index.isin(subj_data_sampled.index)] # Ensures that for multi-animal data, both subjects are retrieved for any sampled time\n",
    "    return subj_data\n",
    "\n",
    "\n",
    "def create_fully_labelled_dataset(session: dict, subj_ids: list = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a fully labelled dataset for all or selected subjects of\n",
    "    a given session dict. Useful to \"bookmark\" frames for use with SLEAP's\n",
    "    predict \"only-labeled-frames\" option.\n",
    "\n",
    "    Args:\n",
    "        session (dict): A session dictionary.\n",
    "        subj_ids (list, optional): A list of subject ids. If None, all\n",
    "            subjects are selected.\n",
    "    Returns:\n",
    "        pandas.DataFrame: A pandas DataFrame containing pos tracking,\n",
    "            video data, and subject ID.\n",
    "    \"\"\"\n",
    "    all_subj_data = pd.DataFrame()\n",
    "    if not subj_ids:\n",
    "        subj_ids = session[\"subjects\"].keys()\n",
    "    for subj in subj_ids:\n",
    "        root = session.get(\"root\", session[\"subjects\"][subj].get(\"root\"))\n",
    "        subj_data = get_raw_tracking_data(\n",
    "            root,\n",
    "            subj,\n",
    "            session[\"subjects\"][subj][\"start\"],\n",
    "            session[\"subjects\"][subj][\"end\"],\n",
    "            source_reader=exp02.CameraTop.Video,\n",
    "        )\n",
    "        all_subj_data = pd.concat([all_subj_data, subj_data])\n",
    "\n",
    "    return all_subj_data\n",
    "\n",
    "def transform_coordinates(homography_matrix: np.ndarray, coordinate: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Transforms coordinates using a homography matrix.\n",
    "\n",
    "    Args:\n",
    "        homography_matrix (numpy.ndarray): The homography matrix.\n",
    "        coordinate (numpy.ndarray): The coordinates to transform.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The transformed coordinates.\n",
    "    \"\"\"\n",
    "    homography_matrix = np.linalg.inv(homography_matrix)\n",
    "    # Convert to homogeneous coordinate\n",
    "    coordinate = np.append(coordinate, 1)\n",
    "    # Transform the coordinate\n",
    "    transformed_coordinate = homography_matrix @ coordinate\n",
    "    # Convert back to cartesian coordinate\n",
    "    transformed_coordinate = transformed_coordinate[:2] / transformed_coordinate[2]\n",
    "    return np.array(transformed_coordinate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries for each session\n",
    "aeon3_social02 = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON3/social0.2/\",\n",
    "    \"working_dir\": \"/ceph/aeon/aeon/code/scratchpad/sleap/multi_point_tracking/multi_animal_CameraNSEW/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104045\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-01-31 12:00:00\"),\n",
    "        },\n",
    "        \"BAA-1104047\": {\n",
    "            \"start\": pd.Timestamp(\"2024-02-05 16:00:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2024-02-10 11:00:00\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"aeon3_social02\",\n",
    "}\n",
    "\n",
    "aeon3_social02_EVAL = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON3/social0.2/\",\n",
    "    \"working_dir\": \"/ceph/aeon/aeon/code/scratchpad/sleap/multi_point_tracking/multi_animal_CameraNSEW/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104045\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-02-25 18:00:00\"),\n",
    "        },\n",
    "        \"BAA-1104047\": {\n",
    "            \"start\": pd.Timestamp(\"2024-02-28 15:00:00\"),\n",
    "        },\n",
    "        # \"multi_animal\": {\n",
    "        #     \"start\": pd.Timestamp(\"2024-02-17 17:00:00\"),\n",
    "        # },\n",
    "    },\n",
    "    \"session\": \"aeon3_social02_EVAL\",\n",
    "}\n",
    "\n",
    "aeon4_social02 = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON4/social0.2/\",\n",
    "    \"working_dir\": \"/ceph/aeon/aeon/code/scratchpad/sleap/multi_point_tracking/multi_animal_CameraNSEW/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104048\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-02-28 11:00:00\"), \n",
    "        },\n",
    "        \"BAA-1104049\": {\n",
    "            \"start\": pd.Timestamp(\"2024-02-05 17:00:00\"), # \n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2024-02-10 11:00:00\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"aeon4_social02\",\n",
    "}\n",
    "\n",
    "aeon4_social02_EVAL = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON4/social0.2/\",\n",
    "    \"working_dir\": \"/ceph/aeon/aeon/code/scratchpad/sleap/multi_point_tracking/multi_animal_CameraNSEW/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104048\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-02-25 18:00:00\"),\n",
    "        },\n",
    "        \"BAA-1104049\": {\n",
    "            \"start\": pd.Timestamp(\"2024-02-28 15:00:00\"),\n",
    "        },\n",
    "        # \"multi_animal\": {\n",
    "        #     \"start\": pd.Timestamp(\"2024-02-17 12:00:00\"),\n",
    "        # },\n",
    "    },\n",
    "    \"session\": \"aeon4_social02_EVAL\",\n",
    "}\n",
    "\n",
    "aeon3_social03 = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON3/social0.3/\",\n",
    "    \"working_dir\": \"/ceph/aeon/aeon/code/scratchpad/sleap/multi_point_tracking/multi_animal_CameraNSEW/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104516\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-07-07 16:00:00\"),\n",
    "        },\n",
    "        \"BAA-1104519\": {\n",
    "            \"start\": pd.Timestamp(\"2024-06-10 15:00:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2024-06-25 18:00:00\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"aeon3_social03\",\n",
    "}\n",
    "\n",
    "aeon3_social03_EVAL = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON3/social0.3/\",\n",
    "    \"working_dir\": \"/ceph/aeon/aeon/code/scratchpad/sleap/multi_point_tracking/multi_animal_CameraNSEW/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104516\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-07-10 11:00:00\"),\n",
    "        },\n",
    "        \"BAA-1104519\": {\n",
    "            \"start\": pd.Timestamp(\"2024-07-14 11:00:00\"),\n",
    "        },\n",
    "        # \"multi_animal\": {\n",
    "        #     \"start\": pd.Timestamp(\"2024-07-05 12:00:00\"),\n",
    "        # },\n",
    "    },\n",
    "    \"session\": \"aeon3_social03_EVAL\",\n",
    "}\n",
    "\n",
    "aeon4_social03 = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON4/social0.3/\",\n",
    "    \"working_dir\": \"/ceph/aeon/aeon/code/scratchpad/sleap/multi_point_tracking/multi_animal_CameraNSEW/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104568\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-07-04 14:00:00\"),\n",
    "        },\n",
    "        \"BAA-1104569\": {\n",
    "            \"start\": pd.Timestamp(\"2024-06-09 13:00:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2024-06-30 11:00:00\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"aeon4_social03\",\n",
    "}\n",
    "\n",
    "aeon4_social03_EVAL = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON4/social0.3/\",\n",
    "    \"working_dir\": \"/ceph/aeon/aeon/code/scratchpad/sleap/multi_point_tracking/multi_animal_CameraNSEW/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104568\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-07-08 13:00:00\"),\n",
    "        },\n",
    "        \"BAA-1104569\": {\n",
    "            \"start\": pd.Timestamp(\"2024-07-13 10:00:00\"),\n",
    "        },\n",
    "        # \"multi_animal\": {\n",
    "        #     \"start\": pd.Timestamp(\"2024-07-02 13:00:00\"),\n",
    "        # },\n",
    "    },\n",
    "    \"session\": \"aeon4_social03_EVAL\",\n",
    "}\n",
    "\n",
    "aeon3_social04 = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON3/social0.4/\",\n",
    "    \"working_dir\": \"/ceph/aeon/aeon/code/scratchpad/sleap/multi_point_tracking/multi_animal_CameraNSEW/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104792\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-09-13 09:00:00\"),\n",
    "        },\n",
    "        \"BAA-1104794\": {\n",
    "            \"start\": pd.Timestamp(\"2024-09-22 09:00:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2024-08-28 14:00:00\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"aeon3_social04\",\n",
    "}\n",
    "\n",
    "aeon3_social04_EVAL = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON3/social0.4/\",\n",
    "    \"working_dir\": \"/ceph/aeon/aeon/code/scratchpad/sleap/multi_point_tracking/multi_animal_CameraNSEW/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104792\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-09-09 18:00:00\"),\n",
    "        },\n",
    "        \"BAA-1104794\": {\n",
    "            \"start\": pd.Timestamp(\"2024-09-17 14:00:00\"),\n",
    "        },\n",
    "        # \"multi_animal\": {\n",
    "        #     \"start\": pd.Timestamp(\"2024-09-08 11:00:00\"),\n",
    "        # },\n",
    "    },\n",
    "    \"session\": \"aeon3_social04_EVAL\",\n",
    "}\n",
    "\n",
    "aeon4_social04 = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON4/social0.4/\",\n",
    "    \"working_dir\": \"/ceph/aeon/aeon/code/scratchpad/sleap/multi_point_tracking/multi_animal_CameraNSEW/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104795\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-08-16 17:00:00\"),\n",
    "        },\n",
    "        \"BAA-1104797\": {\n",
    "            \"start\": pd.Timestamp(\"2024-08-20 11:00:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2024-08-28 14:00:00\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"aeon4_social04\",\n",
    "}\n",
    "\n",
    "aeon4_social04_EVAL = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON4/social0.4/\",\n",
    "    \"working_dir\": \"/ceph/aeon/aeon/code/scratchpad/sleap/multi_point_tracking/multi_animal_CameraNSEW/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104795\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-09-09 16:00:00\"),\n",
    "        },\n",
    "        \"BAA-1104797\": {\n",
    "            \"start\": pd.Timestamp(\"2024-09-17 13:00:00\"),\n",
    "        },\n",
    "        # \"multi_animal\": {\n",
    "        #     \"start\": pd.Timestamp(\"2024-09-08 11:00:00\"),\n",
    "        # },\n",
    "    },\n",
    "    \"session\": \"aeon4_social04_EVAL\",\n",
    "}\n",
    "\n",
    "session = aeon3_social03\n",
    "session_EVAL = aeon3_social03_EVAL\n",
    "for sess in [session, session_EVAL]:\n",
    "    for subj in sess[\"subjects\"]:\n",
    "        sess[\"subjects\"][subj][\"end\"] = pd.Timestamp(sess[\"subjects\"][subj][\"start\"]) + pd.Timedelta(\"1h\")\n",
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract single + multi-animal frames\n",
    "all_subj_data = create_session_dataset(session)\n",
    "overview = all_subj_data.groupby([\"id\", \"identity_name\"]).count()\n",
    "display(overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_data_copy = all_subj_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_data = all_subj_data_copy.copy()\n",
    "fig = plot_position_histograms(all_subj_data)\n",
    "overview = all_subj_data.groupby([\"id\", \"identity_name\"]).count()\n",
    "display(overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_added = False\n",
    "for subj in session[\"subjects\"]:\n",
    "    if \"multi_\" in subj and overview.loc[subj].iloc[0][0] < 1000:\n",
    "        frames_added = True\n",
    "        print(\"Not enough multi-animal frames, adding more...\")\n",
    "        subj_dict = {\n",
    "            \"id\": \"multi_animal\",\n",
    "            \"root\": session[\"root\"],\n",
    "            \"start\": session[\"subjects\"][\"multi_animal\"][\"start\"],\n",
    "            \"end\": session[\"subjects\"][\"multi_animal\"][\"end\"],\n",
    "        }\n",
    "        # adjust n_samples and n_bins to get the desired number of frames\n",
    "        subj_data = create_subject_dataset(\n",
    "            subj_dict,\n",
    "            max_dist = 600,\n",
    "            n_samples=1,\n",
    "            n_bins=30,\n",
    "        )\n",
    "        all_subj_data = pd.concat([all_subj_data, subj_data])\n",
    "        # drop duplicates\n",
    "        all_subj_data.drop_duplicates(inplace=True)\n",
    "    elif overview.loc[subj].iloc[0][0] < 1000:\n",
    "        frames_added = True\n",
    "        print(f\"Not enough frames for {subj}, adding more...\")\n",
    "        subj_dict = {\n",
    "            \"id\": subj,\n",
    "            \"root\": session[\"root\"],\n",
    "            \"start\": session[\"subjects\"][subj][\"start\"],\n",
    "            \"end\": session[\"subjects\"][subj][\"end\"],\n",
    "        }\n",
    "        # adjust n_samples and n_bins to get the desired number of frames\n",
    "        if overview.loc[subj].iloc[0][0] > 700:\n",
    "            subj_data = create_subject_dataset(\n",
    "                subj_dict,\n",
    "                n_samples=2,\n",
    "                n_bins=35\n",
    "            )\n",
    "        else: # sample more\n",
    "            subj_data = create_subject_dataset(\n",
    "                subj_dict,\n",
    "                n_samples=2,\n",
    "                n_bins=55\n",
    "            )\n",
    "        all_subj_data = all_subj_data[all_subj_data[\"id\"] != subj]\n",
    "        all_subj_data = pd.concat([all_subj_data, subj_data])\n",
    "if frames_added:\n",
    "    fig = plot_position_histograms(all_subj_data)\n",
    "    overview = all_subj_data.groupby([\"id\", \"identity_name\"]).count()\n",
    "    display(overview)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional sanity check\n",
    "# # Choose f so that it is one of the frames from all_subj_data\n",
    "# from aeon.io.video import frames\n",
    "# from aeon.analysis.movies import gridframes\n",
    "# import plotly.express as px\n",
    "# f = 162681\n",
    "# fig = px.imshow(gridframes(list(frames(all_subj_data[all_subj_data['_frame'] == f])), width=1440, height=1080, shape=1))\n",
    "# fig.add_trace(px.scatter(all_subj_data[all_subj_data['_frame'] == f], x='x', y='y').data[0])\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition_computer = session[\"root\"].split(\"/\")[-3]\n",
    "experiment = session[\"root\"].split(\"/\")[-2]\n",
    "experiment_no_period = experiment.replace(\".\", \"\")\n",
    "quadrant_cameras = ['CameraSouth', 'CameraNorth', 'CameraEast', 'CameraWest']\n",
    "# Paths for homographies\n",
    "homography_paths = [f'/ceph/aeon/aeon/code/scratchpad/Orsi/pixel_mapping/pixel_mapping_results/{experiment}/{acquisition_computer}/H_{camera}.npy' \n",
    "                    for camera in quadrant_cameras]\n",
    "# Load homographies\n",
    "homographies = [np.load(path) for path in homography_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_data_composite_vid = {'time': [], 'x': [], 'y': [], 'identity_name': [], 'distance': [], '_frame': [], '_path': [], 'id': []}\n",
    "for subject in session[\"subjects\"]:\n",
    "    subj_data = all_subj_data[all_subj_data[\"id\"] == subject]\n",
    "    start = session[\"subjects\"][subject][\"start\"].strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "    composite_vid_frames_info = pd.read_csv(f'{session[\"working_dir\"]}{acquisition_computer}_{experiment_no_period}_{start}_composite_vid_frames_info.csv', index_col=0)\n",
    "    for _, row in subj_data.iterrows():\n",
    "        # Find the corresponding frames in the composite video (there can be >1 if the mice are far apart)\n",
    "        frame = row[\"_frame\"]\n",
    "        composite_vid_frame_info = composite_vid_frames_info[composite_vid_frames_info[\"_frame\"] == frame]\n",
    "        num_quadrant_frames = composite_vid_frame_info.shape[0]\n",
    "        i = 0\n",
    "        for _, composite_vid_frame_info_row in composite_vid_frame_info.iterrows():\n",
    "            # Find the quadrant camera for the frame\n",
    "            camera = composite_vid_frame_info_row[\"_path\"].split(\"/\")[-2]\n",
    "            # Find the homography for the camera\n",
    "            homography = homographies[quadrant_cameras.index(camera)]\n",
    "            # Convert the top camera x and y to the quadrant camera x and y \n",
    "            transformed_coordinate = transform_coordinates(homography, np.array([row[\"x\"], row[\"y\"]]))\n",
    "            x = transformed_coordinate[0]\n",
    "            y = transformed_coordinate[1]\n",
    "            # If outside the quadrant camera dimensions, skip\n",
    "            if x < 0 or x > 1440 or y < 0 or y > 1080:\n",
    "                i+=1\n",
    "                warnings.warn(f'Coordinate {row[\"x\"], row[\"y\"]} on frame {row[\"_frame\"]} for subject {subject} is outside the dimensions of the {camera} camera')\n",
    "                # If outside the dimensions of all quadrant cameras, raise a warning\n",
    "                # if i == num_quadrant_frames:\n",
    "                #     warnings.warn(f'Coordinate {row[\"x\"], row[\"y\"]} on frame {row[\"_frame\"]} is outside the dimensions of the quadrant cameras')\n",
    "                continue\n",
    "            # Else, append to the all_subj_data_composite_vid df\n",
    "            all_subj_data_composite_vid[\"time\"].append(row.name)\n",
    "            all_subj_data_composite_vid[\"x\"].append(x)\n",
    "            all_subj_data_composite_vid[\"y\"].append(y)\n",
    "            all_subj_data_composite_vid[\"identity_name\"].append(row[\"identity_name\"])\n",
    "            all_subj_data_composite_vid[\"distance\"].append(row[\"distance\"])\n",
    "            all_subj_data_composite_vid[\"_frame\"].append(composite_vid_frame_info_row.name) # append(composite_vid_frame_info_row[\"_frame\"])\n",
    "            all_subj_data_composite_vid[\"_path\"].append(f'{session[\"working_dir\"]}{acquisition_computer}_{experiment_no_period}_{start}_composite_video.avi') # append(composite_vid_frame_info_row[\"_path\"])\n",
    "            all_subj_data_composite_vid[\"id\"].append(row[\"id\"])\n",
    "            \n",
    "all_subj_data_composite_vid = pd.DataFrame(all_subj_data_composite_vid).set_index('time')\n",
    "display(all_subj_data_composite_vid.sort_values(by='_frame'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional sanity check\n",
    "# # Since extracting frames from the composite vids fails for some reason, you need to change a couple of lines in the code above for this to work\n",
    "# # Append composite_vid_frame_info_row[\"_frame\"] to all_subj_data_composite_vid[\"_frame\"]\n",
    "# # And append composite_vid_frame_info_row[\"_path\"] to all_subj_data_composite_vid[\"_path\"]\n",
    "# fig = px.imshow(gridframes(list(frames(all_subj_data_composite_vid[all_subj_data_composite_vid['_frame'] == f].iloc[0:1])), width=1440, height=1080, shape=1))\n",
    "# fig.add_trace(px.scatter(all_subj_data_composite_vid[all_subj_data_composite_vid['_frame'] == f].iloc[0:1], x='x', y='y').data[0])\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "all_subj_data_composite_vid.to_csv(f'{session[\"working_dir\"]}{session[\"session\"]}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fully labelled datasets for SLEAP model validation\n",
    "data = create_fully_labelled_dataset(\n",
    "    session_EVAL,\n",
    "    subj_ids=[\n",
    "        subj for subj in session_EVAL[\"subjects\"].keys()\n",
    "    ],\n",
    ").sample(frac=0.1)  # sample only 10% of the data\n",
    "display(data)\n",
    "data.to_csv(f'{session_EVAL[\"working_dir\"]}{session_EVAL[\"session\"]}_frames_top_cam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_vid_data = {'time': [], 'x': [], 'y': [], 'identity_name': [], 'distance': [], '_frame': [], '_path': [], 'id': []}\n",
    "for subject in data[\"id\"].unique():\n",
    "    subj_data = data[data[\"id\"] == subject]\n",
    "    start = session_EVAL[\"subjects\"][subject][\"start\"].strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "    composite_vid_frames_info = pd.read_csv(f'{session_EVAL[\"working_dir\"]}{acquisition_computer}_{experiment_no_period}_{start}_composite_vid_frames_info.csv', index_col=0)\n",
    "    for _, row in subj_data.iterrows():\n",
    "        # Find the corresponding frames in the composite video (there can be >1 if the mice are far apart)\n",
    "        frame = row[\"_frame\"]\n",
    "        composite_vid_frame_info = composite_vid_frames_info[composite_vid_frames_info[\"_frame\"] == frame]\n",
    "        num_quadrant_frames = composite_vid_frame_info.shape[0]\n",
    "        i = 0\n",
    "        for _, composite_vid_frame_info_row in composite_vid_frame_info.iterrows():\n",
    "            # Find the quadrant camera for the frame\n",
    "            camera = composite_vid_frame_info_row[\"_path\"].split(\"/\")[-2]\n",
    "            # Find the homography for the camera\n",
    "            homography = homographies[quadrant_cameras.index(camera)]\n",
    "            # Convert the top camera x and y to the quadrant camera x and y \n",
    "            transformed_coordinate = transform_coordinates(homography, np.array([row[\"x\"], row[\"y\"]]))\n",
    "            x = transformed_coordinate[0]\n",
    "            y = transformed_coordinate[1]\n",
    "            # If outside the quadrant camera dimensions, skip\n",
    "            if x < 0 or x > 1440 or y < 0 or y > 1080:\n",
    "                i+=1\n",
    "                # If outside the dimensions of all quadrant cameras, raise a warning\n",
    "                if i == num_quadrant_frames:\n",
    "                    warnings.warn(f'Coordinate {row[\"x\"], row[\"y\"]} on frame {row[\"_frame\"]} is outside the dimensions of the quadrant cameras')\n",
    "                continue\n",
    "            # Else, append to the all_subj_data_composite_vid df\n",
    "            composite_vid_data[\"x\"].append(x)\n",
    "            composite_vid_data[\"y\"].append(y)\n",
    "            composite_vid_data[\"identity_name\"].append(row[\"identity_name\"])\n",
    "            composite_vid_data[\"time\"].append(row.name)\n",
    "            composite_vid_data[\"distance\"].append(row[\"distance\"])\n",
    "            composite_vid_data[\"_frame\"].append(composite_vid_frame_info_row.name) \n",
    "            composite_vid_data[\"_path\"].append(f'{session_EVAL[\"working_dir\"]}{acquisition_computer}_{experiment_no_period}_{start}_composite_video.avi') \n",
    "            composite_vid_data[\"id\"].append(row[\"id\"])\n",
    "    \n",
    "composite_vid_data = pd.DataFrame(composite_vid_data).set_index('time')\n",
    "composite_vid_data.to_csv(f'{session_EVAL[\"working_dir\"]}{session_EVAL[\"session\"]}_frames.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import sleap\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict\n",
    "from sleap.io.pathutils import fix_path_separator\n",
    "from sleap.gui.suggestions import VideoFrameSuggestions\n",
    "from sleap.nn.config import *\n",
    "from sleap.nn.inference import TopDownMultiClassPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_tracks(labels: sleap.Labels) -> sleap.Labels:\n",
    "    \"\"\"\n",
    "    Removes instances from SLEAP Labels object where track = None.\n",
    "\n",
    "    Args:\n",
    "        labels (sleap.Labels): A SLEAP Labels object.\n",
    "    Returns:\n",
    "        sleap.Labels: A SLEAP Labels object with only tracked instances in each frame.\n",
    "    \"\"\"\n",
    "    lfs = [lf.remove_untracked() for lf in labels.labeled_frames]\n",
    "    return sleap.Labels(\n",
    "        labeled_frames=lfs,\n",
    "        videos=labels.videos,\n",
    "        skeletons=labels.skeletons,\n",
    "        tracks=labels.tracks,\n",
    "    )\n",
    "\n",
    "def generate_slp_dataset(\n",
    "    subj_data: pd.DataFrame,\n",
    "    skeleton: sleap.Skeleton,\n",
    "    tracks_dict: Optional[Dict[str, sleap.Track]] = None,\n",
    ") -> sleap.Labels:\n",
    "    \"\"\"\n",
    "    Generates .slp dataset for a given session dict.\n",
    "\n",
    "    Args:\n",
    "        subj_data (pandas.DataFrame): A pandas DataFrame containing the labeled data for a given session.\n",
    "        skeleton (sleap.Skeleton): A sleap Skeleton object.\n",
    "        tracks_dict (dict, optional): A dictionary containing track names and their corresponding sleap Track objects.\n",
    "            If None, a new dictionary is created from the subject IDs in the input data. Default is None.\n",
    "    Returns:\n",
    "        sleap.Labels: A SLEAP Labels object containing labeled frames.\n",
    "    \"\"\"\n",
    "\n",
    "    # create tracks dictionary from subj_ids that are not multi_animal\n",
    "    if not tracks_dict:\n",
    "        tracks_dict = {\n",
    "            subj: sleap.Track(spawned_on=0, name=subj)\n",
    "            for subj in subj_data[\"id\"].unique()\n",
    "            if \"multi_\" not in subj\n",
    "        }\n",
    "\n",
    "    lfs = []\n",
    "\n",
    "    # create video dictionary from new labels\n",
    "    videos_dict = {\n",
    "        video: sleap.Video.from_filename(video, grayscale=True)\n",
    "        for video in subj_data._path.unique()\n",
    "    }\n",
    "\n",
    "    for name, group in subj_data.groupby([\"_path\", \"_frame\"]):\n",
    "        instances = []\n",
    "        for _, row in group.iterrows():\n",
    "            instances.append(\n",
    "                sleap.Instance(\n",
    "                    skeleton=skeleton,\n",
    "                    track=tracks_dict[row.identity_name],\n",
    "                    points={\"centroid\": sleap.instance.Point(row.x, row.y)},\n",
    "                )\n",
    "            )\n",
    "        lf = sleap.instance.LabeledFrame(\n",
    "            video=videos_dict[name[0]],\n",
    "            frame_idx=name[1],\n",
    "            instances=instances,\n",
    "        )\n",
    "        lfs.append(lf)\n",
    "\n",
    "    return sleap.Labels(labeled_frames=lfs)\n",
    "\n",
    "\n",
    "def update_slp_video_paths(\n",
    "    labels: \"sleap.Labels\", old_path: str, new_path: str\n",
    ") -> sleap.Labels:\n",
    "    \"\"\"\n",
    "    Updates video paths in a SLEAP labels object (e.g., to move training from local to remote machine).\n",
    "\n",
    "    Args:\n",
    "        labels (sleap.Labels): A SLEAP Labels object.\n",
    "        old_path (str): Old path to video files.\n",
    "        new_path (str): New path to video files.\n",
    "\n",
    "    Returns:\n",
    "        sleap.Labels: A SLEAP Labels object with updated video paths.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    videos = [\n",
    "        sleap.Video.from_filename(\n",
    "            fix_path_separator(vid.filename).replace(old_path, new_path), grayscale=True\n",
    "        )\n",
    "        for vid in labels.videos\n",
    "    ]\n",
    "    print(labels.videos)\n",
    "    print(fix_path_separator(labels.videos[0].filename).replace(old_path, new_path))\n",
    "    print(sleap.Video.from_filename(\n",
    "            fix_path_separator(labels.videos[0].filename).replace(old_path, new_path), grayscale=True\n",
    "        ))\n",
    "\n",
    "    lfs = []\n",
    "    for lf in labels.labeled_frames:\n",
    "        lf = sleap.instance.LabeledFrame(\n",
    "            video=videos[labels.videos.index(lf.video)],\n",
    "            frame_idx=lf.frame_idx,\n",
    "            instances=lf.instances,\n",
    "        )\n",
    "        lfs.append(lf)\n",
    "\n",
    "    return sleap.Labels(\n",
    "        labeled_frames=lfs,\n",
    "        videos=videos,\n",
    "        skeletons=labels.skeletons,\n",
    "        tracks=labels.tracks,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new skeleton\n",
    "skeleton = sleap.Skeleton()\n",
    "skeleton.add_node(\"centroid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate slp training dataset for all subjects\n",
    "session[\"working_dir\"] = session[\"working_dir\"].replace(\"/ceph/aeon/\", \"Z:/\")\n",
    "subj_data = pd.read_csv(f'{session[\"working_dir\"]}{session[\"session\"]}.csv')\n",
    "tracks_dict = {\n",
    "    'BAA-1104045': sleap.Track(spawned_on=0, name='BAA-1104045'),\n",
    "    'BAA-1104047': sleap.Track(spawned_on=0, name='BAA-1104047')\n",
    "}\n",
    "labels = generate_slp_dataset(subj_data, skeleton, tracks_dict)\n",
    "sleap.Labels.save_file(labels, f'{session[\"working_dir\"]}{session[\"session\"]}.slp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual annotation\n",
    "- open the .slp file in the sleap GUI\n",
    "- go through all the videos and adjust any labels that are not on the animal (this happens occasionally since the homography isn't perfect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after manual annotation on local machine, update video paths in labels to point to ceph\n",
    "labels = sleap.Labels.load_file(f'{session[\"session\"]}_labelled.slp')\n",
    "labels = update_slp_video_paths(labels=labels, old_path=\"Z:\", new_path=\"/ceph/aeon\")\n",
    "# labels = update_slp_video_paths(labels=labels, old_path=\"Z:/aeon/code/scratchpad/sleap/multi_point_tracking/multi_animal_CameraNSEW\", new_path='Z:/aeon/code/scratchpad/sleap/multi_point_tracking/multi_animal_CameraNSEW/videos')\n",
    "sleap.Labels.save_file(labels, f'{session[\"session\"]}_ceph.slp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fully labelled datasets for SLEAP model validation\n",
    "session_EVAL[\"working_dir\"] = session_EVAL[\"working_dir\"].replace(\"/ceph/aeon/\", \"Z:/\")\n",
    "csv_files = [\n",
    "    f'{session_EVAL[\"session\"]}_frames.csv',\n",
    "    f'{session_EVAL[\"session\"]}_frames_top_cam.csv',\n",
    "]\n",
    "tracks_dict = {\n",
    "    subj: sleap.Track(spawned_on=0, name=subj)\n",
    "    for subj in session_EVAL[\"subjects\"].keys()\n",
    "    if \"multi_\" not in subj\n",
    "}\n",
    "for csv_file in csv_files:\n",
    "    subj_data = pd.read_csv(csv_file)\n",
    "    labels = generate_slp_dataset(subj_data, skeleton, tracks_dict=tracks_dict)\n",
    "    labels = update_slp_video_paths(labels=labels, old_path=\"Z:\", new_path=\"/ceph/aeon\")\n",
    "    sleap.Labels.save_file(labels, f'{session_EVAL[\"working_dir\"]}{Path(csv_file).stem}_ceph.slp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model on HPC\n",
    "\n",
    "### evaluate\n",
    "- modify `sleap_predict.sh` to predict on EVAL frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate single-animal predictions with ground truth data\n",
    "# matches the metrics used to optimise the sleap models with optuna\n",
    "session_EVAL[\"working_dir\"] = session_EVAL[\"working_dir\"].replace(\"/ceph/aeon/\", \"Z:/\")\n",
    "gt_file = f'{session_EVAL[\"working_dir\"]}{session_EVAL[\"session\"]}_frames_ceph.slp'\n",
    "pr_file = f'{session_EVAL[\"working_dir\"]}predictions/{session_EVAL[\"session\"]}_frames_ceph_pr.slp'\n",
    "labels_gt = sleap.load_file(gt_file)\n",
    "labels_pr = sleap.load_file(pr_file)\n",
    "crop_size = 112 # make this match the crop size used in the centered-instance model\n",
    "\n",
    "track_names = [track.name for track in labels_gt.tracks]\n",
    "max_instances = len(track_names)\n",
    "\n",
    "framepairs = sleap.nn.evals.find_frame_pairs(labels_gt, labels_pr)\n",
    "matches = sleap.nn.evals.match_frame_pairs(framepairs, scale=crop_size)\n",
    "positive_pairs = matches[0]\n",
    "\n",
    "# initialize confusion matrix components\n",
    "total_tp = total_fp = total_fn = total_tn = 0\n",
    "\n",
    "for gt_frame, pr_frame in framepairs:\n",
    "    gt_count = len(gt_frame.instances)\n",
    "    pr_count = len(pr_frame.instances)\n",
    "\n",
    "    if gt_count > max_instances:\n",
    "        raise ValueError(\n",
    "            f\"Ground truth frame {gt_frame.frame_idx} has {gt_count} instances, which is more than the maximum of {max_instances}.\"\n",
    "        )\n",
    "    if pr_count > max_instances:\n",
    "        raise ValueError(\n",
    "            f\"Predicted frame {pr_frame.frame_idx} has {pr_count} instances, which is more than the maximum of {max_instances}.\"\n",
    "        )\n",
    "    \n",
    "    # compute TP, FP, FN, TN for this frame\n",
    "    tp = min(gt_count, pr_count)  # correct detections\n",
    "    fp = max(0, pr_count - gt_count)  # extra detections\n",
    "    fn = max(0, gt_count - pr_count)  # missed detections\n",
    "    tn = max_instances - max(gt_count, pr_count)  # unused \"slots\"\n",
    "\n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "    total_tn += tn\n",
    "\n",
    "# detection metrics\n",
    "detection_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "detection_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "detection_f1_score = (\n",
    "    2 * detection_precision * detection_recall / (detection_precision + detection_recall) if (detection_precision + detection_recall) > 0 else 0\n",
    ")\n",
    "\n",
    "\n",
    "# identity accuracy\n",
    "correct_id = {track_name: 0 for track_name in track_names}\n",
    "total_id_checks = {track_name: 0 for track_name in track_names}\n",
    "\n",
    "for positive_pair in positive_pairs:\n",
    "    gt = (\n",
    "        positive_pair[0]\n",
    "        if isinstance(positive_pair[1], sleap.PredictedInstance)\n",
    "        else positive_pair[1]\n",
    "    )\n",
    "    pr = (\n",
    "        positive_pair[1]\n",
    "        if isinstance(positive_pair[1], sleap.PredictedInstance)\n",
    "        else positive_pair[0]\n",
    "    )\n",
    "    total_id_checks[gt.track.name] += 1\n",
    "    if gt.track.name == pr.track.name:\n",
    "        correct_id[gt.track.name] += 1\n",
    "\n",
    "id_accuracy = (\n",
    "    sum(correct_id.values()) / sum(total_id_checks.values())\n",
    "    if sum(total_id_checks.values()) > 0\n",
    "    else 0.0\n",
    ")\n",
    "\n",
    "# Harmonic mean for composite metric\n",
    "composite_metric = (\n",
    "    (2 * detection_f1_score * id_accuracy) / (detection_f1_score + id_accuracy)\n",
    "    if (detection_f1_score + id_accuracy) > 0\n",
    "    else 0\n",
    ")\n",
    "\n",
    "# print for debugging\n",
    "print(\"Total TP: \", total_tp)\n",
    "print(\"Total FP: \", total_fp)\n",
    "print(\"Total FN: \", total_fn)\n",
    "print(\"Total TN: \", total_tn)\n",
    "print(\"Detection precision: \", round(detection_precision, 3))\n",
    "print(\"Detection recall: \", round(detection_recall, 3))\n",
    "print(\"-\")\n",
    "print(\"Correct ID: \", correct_id)\n",
    "print(\"Total ID checks: \", total_id_checks)\n",
    "print(\"-\")\n",
    "print(\"Detection F1 score: \", round(detection_f1_score, 3))\n",
    "print(\"ID accuracy: \", round(id_accuracy, 3))\n",
    "print(f\"Composite metric (harmonic mean of detection F1 and ID accuracy): {round(composite_metric, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model\n",
    "predictor = TopDownMultiClassPredictor.from_trained_models(\n",
    "    centroid_model_path=f\"{session['working_dir']}/models/{session['session']}_labelled_topdown_top.centroid\",\n",
    "    confmap_model_path=f\"{session['working_dir']}/models/{session['session']}_labelled_topdown_top.centered_instance\",\n",
    "    resize_input_layer=False,  # SLEAP 1.3.0+\n",
    ")\n",
    "\n",
    "predictor.export_model(\n",
    "    \"/ceph/aeon/aeon/code/bonsai-sleap/example_workflows/match_quad_id_to_top_cam_pose/quad_cam_exported_models\"\n",
    "    # max_instances=2,\n",
    "    # unrag_outputs=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "aeon"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
